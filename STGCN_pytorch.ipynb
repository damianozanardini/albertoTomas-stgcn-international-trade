{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ST-GCN for international trade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBJCr3jUun7o",
        "outputId": "dbdb1e61-1549-4db7-feba-5c3ad509c751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from dist_matrix.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Wheter or not we are executing on Google Colab\n",
        "drive = False\n",
        "\n",
        "# Only needed it the code is executed on Colab\n",
        "if drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install tabulate\n",
        "\n",
        "import import_ipynb\n",
        "import dist_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syf0-VKoO41E"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jzjyixNzO41I"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import time\n",
        "import calendar\n",
        "from openpyxl import load_workbook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtUtFdmjv-DO"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bC7crP1qO41L"
      },
      "outputs": [],
      "source": [
        "# Loads data from a CSV file, removing the 'year' column and splitting data into train, validation and test\n",
        "def load_data(file_path, len_train, len_val):\n",
        "    df = pd.read_csv(file_path, header=0).drop(['year'], axis=1).values.astype(float)\n",
        "    train = df[:len_train]\n",
        "    val = df[len_train: len_train + len_val]\n",
        "    test = df[len_train + len_val:]\n",
        "    return train, val, test\n",
        "\n",
        "# Transforms data to prepare input (x) and output (y) to be fed to the model\n",
        "def data_transform(data, n_his, device):\n",
        "    \"\"\"\n",
        "    Transforms data to prepare input (x) and output (y) to be fed to the model.\n",
        "\n",
        "    Args:\n",
        "    data: Data to be manipulated.\n",
        "    n_his (int): Number of past timesteps.\n",
        "    device: The device in which data will be stored.\n",
        "\n",
        "    Returns:\n",
        "    Torch.Tensor, Torch.Tensor: The input and output data.\n",
        "    \"\"\"\n",
        "    n_year = data.shape[0]\n",
        "    # The number of time series (in our setting, the number of countries)\n",
        "    n_series = data.shape[1]\n",
        "    # The number of time windows\n",
        "    n_slot = n_year - n_his\n",
        "\n",
        "    x = np.zeros([n_slot, 1, n_his, n_series])\n",
        "    y = np.zeros([n_slot, n_series])\n",
        "    for i in range(n_slot):\n",
        "        x[i, :, :, :] = data[i:i + n_his].reshape(1, n_his, n_series)\n",
        "        y[i] = data[i + n_his]\n",
        "    return torch.Tensor(x).to(device), torch.Tensor(y).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9NhGKrUwCHM"
      },
      "source": [
        "## STGCN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Architecture of the STGCN model, adapted to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xKAxhhPxQUoO"
      },
      "outputs": [],
      "source": [
        "# stgcn\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class align(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(align, self).__init__()\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        if c_in > c_out:\n",
        "            self.conv1x1 = nn.Conv2d(c_in, c_out, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.c_in > self.c_out:\n",
        "            return self.conv1x1(x)\n",
        "        if self.c_in < self.c_out:\n",
        "            return F.pad(x, [0, 0, 0, 0, 0, self.c_out - self.c_in, 0, 0])\n",
        "        return x\n",
        "\n",
        "class temporal_conv_layer(nn.Module):\n",
        "    def __init__(self, kt, c_in, c_out, act=\"relu\"):\n",
        "        super(temporal_conv_layer, self).__init__()\n",
        "        self.kt = kt\n",
        "        self.act = act\n",
        "        self.c_out = c_out\n",
        "        self.align = align(c_in, c_out)\n",
        "        if self.act == \"GLU\":\n",
        "            self.conv = nn.Conv2d(c_in, c_out * 2, (kt, 1), 1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(c_in, c_out, (kt, 1), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = self.align(x)[:, :, self.kt - 1:, :]\n",
        "        if self.act == \"GLU\":\n",
        "            x_conv = self.conv(x)\n",
        "            return (x_conv[:, :self.c_out, :, :] + x_in) * torch.sigmoid(x_conv[:, self.c_out:, :, :])\n",
        "        if self.act == \"sigmoid\":\n",
        "            return torch.sigmoid(self.conv(x) + x_in)\n",
        "        return torch.relu(self.conv(x) + x_in)\n",
        "\n",
        "class spatio_conv_layer(nn.Module):\n",
        "    def __init__(self, ks, c, Lk):\n",
        "        super(spatio_conv_layer, self).__init__()\n",
        "        self.Lk = Lk\n",
        "        self.theta = nn.Parameter(torch.FloatTensor(c, c, ks))\n",
        "        self.b = nn.Parameter(torch.FloatTensor(1, c, 1, 1))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.theta, a=math.sqrt(5))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.theta)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.b, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_c = torch.einsum(\"knm,bitm->bitkn\", self.Lk, x)\n",
        "        x_gc = torch.einsum(\"iok,bitkn->botn\", self.theta, x_c) + self.b\n",
        "        return torch.relu(x_gc + x)\n",
        "\n",
        "class st_conv_block(nn.Module):\n",
        "    def __init__(self, ks, kt, n, c, p, Lk):\n",
        "        super(st_conv_block, self).__init__()\n",
        "        self.tconv1 = temporal_conv_layer(kt, c[0], c[1], \"GLU\")\n",
        "        self.sconv = spatio_conv_layer(ks, c[1], Lk)\n",
        "        self.tconv2 = temporal_conv_layer(kt, c[1], c[2])\n",
        "        self.ln = nn.LayerNorm([n, c[2]])\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_t1 = self.tconv1(x)\n",
        "        x_s = self.sconv(x_t1)\n",
        "        x_t2 = self.tconv2(x_s)\n",
        "        x_ln = self.ln(x_t2.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "        return self.dropout(x_ln)\n",
        "\n",
        "class fully_conv_layer(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super(fully_conv_layer, self).__init__()\n",
        "        self.conv = nn.Conv2d(c, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class output_layer(nn.Module):\n",
        "    def __init__(self, c, T, n):\n",
        "        super(output_layer, self).__init__()\n",
        "        self.tconv1 = temporal_conv_layer(T, c, c, \"GLU\")\n",
        "        self.ln = nn.LayerNorm([n, c])\n",
        "        self.tconv2 = temporal_conv_layer(1, c, c, \"sigmoid\")\n",
        "        self.fc = fully_conv_layer(c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_t1 = self.tconv1(x)\n",
        "        x_ln = self.ln(x_t1.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "        x_t2 = self.tconv2(x_ln)\n",
        "        return self.fc(x_t2)\n",
        "\n",
        "class STGCN(nn.Module):\n",
        "    def __init__(self, ks, kt, bs, T, n, Lk, p):\n",
        "        super(STGCN, self).__init__()\n",
        "        self.st_conv1 = st_conv_block(ks, kt, n, bs[0], p, Lk)\n",
        "        self.st_conv2 = st_conv_block(ks, kt, n, bs[1], p, Lk)\n",
        "        self.output = output_layer(bs[1][2], T - 4 * (kt - 1), n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_st1 = self.st_conv1(x)\n",
        "        x_st2 = self.st_conv2(x_st1)\n",
        "        return self.output(x_st2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX3jOPo8v3lv"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zia7AGkhQc4e"
      },
      "outputs": [],
      "source": [
        "# Evaluates the model on a data set\n",
        "def evaluate_model(model, loss, data_iter):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a data set.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): Model to be evaluated.\n",
        "    loss (function): Loss function.\n",
        "    data_iter (iterable): Data iterator.\n",
        "\n",
        "    Returns:\n",
        "    float: Average loss.\n",
        "    \"\"\"\n",
        "    model.eval() # Setting the model in evaluation mode\n",
        "    l_sum, n = 0.0, 0\n",
        "\n",
        "    with torch.no_grad(): # Turning off gradients calculus\n",
        "        for x, y in data_iter:\n",
        "            y_pred = model(x).view(len(x), -1)  # Model prediction\n",
        "            l = loss(y_pred, y) # Loss\n",
        "            l_sum += l.item() * y.shape[0] # Accumulating the loss\n",
        "            n += y.shape[0] # Number of examples\n",
        "\n",
        "    return l_sum / n # Average loss\n",
        "\n",
        "# Evaluates the model wrt MAE, MAPE and RMSE \n",
        "def evaluate_metric(model, data_iter, scaler):\n",
        "    \"\"\"\n",
        "    Evaluates the model wrt MAE, MAPE and RMSE.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): Model to be evaluated.\n",
        "    data_iter (iterable): Data iterator.\n",
        "    scaler (scaler object): Objet to de-scale data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: MAE, MAPE and RMSE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Setting the model in evaluation mode\n",
        "\n",
        "    with torch.no_grad(): # Turning off gradients calculus\n",
        "        mae, mape, mse = [], [], []\n",
        "\n",
        "        for x, y in data_iter:\n",
        "            # De-scale ground truth and predictions\n",
        "            y = scaler.inverse_transform(y.cpu().numpy()).reshape(-1)\n",
        "            y_pred = scaler.inverse_transform(model(x).view(len(x), -1).cpu().numpy()).reshape(-1)\n",
        "\n",
        "            d = np.abs(y - y_pred) # Absolute error\n",
        "            mae += d.tolist() # Accumulating MAE\n",
        "            mape += (d / y).tolist() # Accumulating MAE\n",
        "            mse += (d ** 2).tolist() # Accumulating MAE\n",
        "\n",
        "        # Computing final metrics\n",
        "        MAE = np.array(mae).mean() # Mean absolute error\n",
        "        MAPE = np.array(mape).mean() * 100 # Mean percentage absolute error\n",
        "        RMSE = np.sqrt(np.array(mse).mean()) # Square root of mean quadratic error\n",
        "        \n",
        "        return MAE, MAPE, RMSE\n",
        "\n",
        "# For each experiment, a line is written to a local Excel file that contains the main figures:\n",
        "# Type of architecture, number of nodes, hyper-parameters, training time, accuracy, hits@k, MRR...\n",
        "def write_line_to_xlsx(output):\n",
        "    current_GMT = time.gmtime()\n",
        "    time_stamp = calendar.timegm(current_GMT)\n",
        "    time_stamp = datetime.utcfromtimestamp(time_stamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    records = [tuple([time_stamp] + list(output.values()))]\n",
        "\n",
        "    wb = load_workbook(xlsx_path)\n",
        "    # Select First Worksheet\n",
        "    ws = wb.worksheets[0]\n",
        "\n",
        "    for record in records:\n",
        "        # Append Row Values\n",
        "        ws.append(record)\n",
        "\n",
        "    wb.save(xlsx_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5netoO_O41O"
      },
      "source": [
        "## Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mgOQjDdbO41R"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2333)\n",
        "torch.cuda.manual_seed(2333)\n",
        "np.random.seed(2333)\n",
        "random.seed(2333)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebTGhcZfO41T"
      },
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v02RhtCO41U",
        "outputId": "cd6390be-3ece-4d8f-947f-20cfa83967d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# GPU is selected whenever available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    # This is for M-series-based Macs\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzLeIEM1O41V"
      },
      "source": [
        "## File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Fa0p2KvzaBP"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# date-based ID to distinguish models\n",
        "def generate_id():\n",
        "    now = datetime.now()\n",
        "    id_str = now.strftime(\"%Y%m%d%H%M%S%f\") # Formatting date and time with microseconds\n",
        "    return id_str\n",
        "\n",
        "# File paths for data   \n",
        "\n",
        "# The matrix with distances between countries\n",
        "kms_matrix_filename = \"kms_distance.csv\"\n",
        "# The temporal series data\n",
        "data_filename = \"time_series.csv\"\n",
        "# File where the model will be saved\n",
        "save_filename = f'model-{generate_id()[:10]}.pth'\n",
        "# File where results will be saved\n",
        "xlsx_filename = \"tmp_results.xlsx\"\n",
        "\n",
        "main_dir = '/content/drive/My Drive/Colab Notebooks/MUIA/TFM_AlbertoTomasMartin/' if drive else os.getcwd()\n",
        "\n",
        "input_dir = os.path.join(main_dir, \"input\")\n",
        "output_dir = os.path.join(main_dir, \"output\")\n",
        "\n",
        "kms_matrix_path = os.path.join(input_dir,\"matrix\",kms_matrix_filename)\n",
        "data_path = os.path.join(input_dir,data_filename)\n",
        "save_path = os.path.join(output_dir,save_filename)\n",
        "xlsx_path = os.path.join(output_dir,xlsx_filename)\n",
        "\n",
        "dist_matrix_dict = {\n",
        "    \"Kms\": kms_matrix_path\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASxXCcmOQMJs"
      },
      "source": [
        "#### Initial Data Presentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gDgueoItK6rs",
        "outputId": "f79724d1-8388-4c47-ecfd-fac155249247"
      },
      "outputs": [],
      "source": [
        "# Debug only\n",
        "\n",
        "# TO-DO: what is this, exactly?\n",
        "# Is this just the ECI by country by year? In this case, the dimensionality given below (probably taken from Marcos' TFM) is wrong\n",
        "\n",
        "# [N years] x [N countries x N countries]\n",
        "data_df = pd.read_csv(data_path)\n",
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Fcs6zZQQBx"
      },
      "source": [
        " #### SITC Original Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3BnKRn-OQS0x"
      },
      "outputs": [],
      "source": [
        "# This data can be dowloaded from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/H8SFD2\n",
        "stata_file_path = os.path.join(input_dir, 'SITC/country_partner_sitcproduct2digit_year.dta')\n",
        "df = pd.read_stata(stata_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug only\n",
        "print(df.head())\n",
        "print(f\"\\nThe SITC dataset has {len(df)} rows and {len(df.columns)} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug only\n",
        "df[df.location_code==\"AFG\"][df.year==2018]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYYw1guWO41X"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ASPqy8E4O41Z"
      },
      "outputs": [],
      "source": [
        "# From a total of 60 years (timesteps) of data, we split into train, validation and test.\n",
        "# This means that\n",
        "# - the first n_train years are used for training\n",
        "# - the following n_val years are used for validation\n",
        "# - the rest are used for test\n",
        "# \n",
        "# Given each of these splits, data are generated as follows: a slice of n_his+1 consecutive years is taken, from which the first n_his are x, and the last one is y (values to be predicted)\n",
        "#\n",
        "# Example with n_train=35, n_his=10: since the training years span from 1962 to 1996 (corresponding to 35 timesteps), we can take 25 slices of 10+1=11 years [1962..1972], [1963..1973], .., [1986..1996].git.\n",
        "# For each slice, the first 10 years are x, and the last year will be y\n",
        "\n",
        "# n_his = 10 # Number of timesteps to be considered as input\n",
        "\n",
        "# Conditions:\n",
        "# - n_train+n_val+n_test = total number of years (timesteps) in the dataset\n",
        "# - n_train >= n_his+1\n",
        "# - n_val >= n_his+1\n",
        "# - n_test >= n_his+1\n",
        "#n_train = 35 # Number of examples at training\n",
        "#n_val = 12 # Number of examples at validation\n",
        "#n_test = 13 # Number of examples at test\n",
        "\n",
        "# Ks, Kt = 4, 3 # Size of the kernel for the spatial and the temporal convolution, respectively\n",
        "\n",
        "blocks = [[1, 512, 1024], [1024, 1024, 2048]] # Structure of blocks for the temporal and spatial convolution:\n",
        "# The first list corresponds to input and output channels of the first two STConv blocks\n",
        "# The second list corresponds to input and output channels of the third STConv block\n",
        "\n",
        "drop_prob = 0 # Dropout probability (used to regularize the model and avoid overfitting)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "# epochs = 50\n",
        "# lr = 1e-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQMUr0E6O41a"
      },
      "source": [
        "## Building the graph\n",
        "\n",
        "We explore several option for distance, starting from the original matrix on the combination of geographical distance between the main cities of two countries (the ECI is computed using the SITC product data from 1962 to 2021, while the adjacency matrix considers bilateral distances between major cities weighted by population shares)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug only\n",
        "option = \"Reciprocal\" # [\"Raw\", \"Random\", \"Constant\", \"Laplacian\", \"Reciprocal\", \"Exponential reciprocal\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci5iC4IOzyw7"
      },
      "source": [
        "# Training & Saving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7dPL0rQVxxk"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DLCt0LGO41c"
      },
      "source": [
        "### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initializing a standard scaler to normalize data\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L4TAAlMYO41e"
      },
      "outputs": [],
      "source": [
        "def get_input_data(n_train, n_val, n_test, n_his):\n",
        "    # Loading data from file and splitting into train, validation and test\n",
        "    train_df, val_df, test_df = load_data(data_path, n_train , n_val)\n",
        "\n",
        "    train = scaler.fit_transform(train_df)\n",
        "    val = scaler.transform(val_df)\n",
        "    test = scaler.transform(test_df)\n",
        "\n",
        "    x_train, y_train = data_transform(train, n_his, device)\n",
        "    x_val, y_val = data_transform(val, n_his, device)\n",
        "    x_test, y_test = data_transform(test, n_his, device)\n",
        "\n",
        "    # Creating a PyTorch DataLoader for train data, that allows iterating on batches\n",
        "    train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "    train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=False)\n",
        "    val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "    val_iter = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False)\n",
        "    test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "    test_iter = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
        "\n",
        "    return train_iter, val_iter, test_iter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug only\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TGRAa8nVxxl",
        "outputId": "907c5d1d-7318-4027-cf78-dcbd7db5e182"
      },
      "outputs": [],
      "source": [
        "# Debug only\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b818OHe7O41p"
      },
      "source": [
        "## Loss & Model & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bqYpbIC2UdaS"
      },
      "outputs": [],
      "source": [
        "# Loss function: MSE\n",
        "loss = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimizer(optimizer_index, model, lr):\n",
        "    # Defining optimizer from a dictionary\n",
        "    optimizers = {\n",
        "        \"RMSprop\" : torch.optim.RMSprop(model.parameters(), lr=lr), # Optimization using RMSprop wrt the specified learning rate\n",
        "        \"AdamW\" : torch.optim.AdamW(model.parameters(), lr=lr)    # Optimization using AdamW wrt the specified learning rate\n",
        "    }\n",
        "    return optimizers[optimizer_index]\n",
        "\n",
        "def get_scheduler(scheduler_index, optimizer, epochs):\n",
        "    # Scheduler dictionary\n",
        "    schedulers = {\n",
        "        # Reduces the learning rate by gamma every step_size epochs\n",
        "        \"StepLR\": torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.6),\n",
        "        # Reduces the learning rate exponencially on gamma\n",
        "        \"ExponentialLR\": torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99),\n",
        "        # Adjusts the learning rate following a cosine function, with minimum eta_min\n",
        "        \"CosineAnnealingLR\": torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "    }\n",
        "    return schedulers[scheduler_index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0vjjaD8O41r"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FAcKoPG3O41s",
        "outputId": "d572dec6-988b-4444-cf47-dbb1cd98cfea"
      },
      "outputs": [],
      "source": [
        "def train(model,epochs,train_iter,val_iter):\n",
        "    patience = 4\n",
        "\n",
        "    # Initializing variables\n",
        "    min_train_loss = np.inf # Best observed training loss\n",
        "    min_val_loss = np.inf # Best observed validation loss\n",
        "    best_epoch_train = 0 # Epoch when the best training loss is observed\n",
        "    best_epoch_val = 0 # Epoch when the best validation loss is observed\n",
        "    train_loss_list, val_loss_list = [], [] # Lists to store traininng and validation loss through epochs\n",
        "\n",
        "    # Initializing lists\n",
        "    last_val_losses = [np.inf] * patience # Current validation losses for comparison\n",
        "\n",
        "    # Initializing the model state\n",
        "    best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f\"Training for {epochs} epochs\")\n",
        "    # Main training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        l_sum = 0.0 # Initializing loss sum\n",
        "        n = 0\n",
        "        model.train()\n",
        "\n",
        "        # Loop on training data\n",
        "        for x, y in train_iter:\n",
        "            y_pred = model(x).view(len(x), -1) # Model prediction\n",
        "            l = loss(y_pred, y) # Loss\n",
        "            optimizer.zero_grad() # Clean gradients\n",
        "            l.backward() # Back-propagation\n",
        "            optimizer.step() # Update parameters\n",
        "            l_sum += l.item() * y.shape[0] # Accumulating loss\n",
        "            n += y.shape[0] # Number of examples so far # TO-DO: is 24 a meaningful number?\n",
        "\n",
        "        scheduler.step() # Update learning rate according to the scheduler\n",
        "\n",
        "        # Evaluating the model on the validation set\n",
        "        val_loss = evaluate_model(model, loss, val_iter)\n",
        "\n",
        "        train_loss = l_sum / n # Average training loss\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "\n",
        "        # Possibly updating the best training loss\n",
        "        if train_loss < min_train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            best_epoch_train = epoch\n",
        "\n",
        "        if epoch >= patience:\n",
        "            # Keeping the last validation losses\n",
        "            for i in reversed(1,range(patience)):\n",
        "                last_val_losses[i] = last_val_losses[i-1]\n",
        "            last_val_losses[0] = val_loss\n",
        "\n",
        "            if all(min_val_loss < c for c in last_val_losses):\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "            else:\n",
        "                min_val_loss = min(last_val_losses)\n",
        "                best_epoch_val = epoch\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            # Possibly updating the best validation losses and saving the model state\n",
        "            #if all(c < b for c, b in zip(last_val_losses, best_val_losses)):\n",
        "            #    best_val_losses = last_val_losses.copy()\n",
        "            #    min_val_loss = val_loss\n",
        "            #    best_epoch_val = epoch\n",
        "            #    best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Printing the current learning rate and losses, the first epoch and after each 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch} -> Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    # After training is complete, loading the best model state\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    return min_train_loss, best_epoch_train, min_val_loss, best_epoch_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt11Kh--O41u"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgxa24i2O41v",
        "outputId": "65b96041-87c5-44b4-dff4-adb1e07006ed"
      },
      "outputs": [],
      "source": [
        "def test(model,test_iter):\n",
        "    l = evaluate_model(model, loss, test_iter)\n",
        "    MAE, MAPE, RMSE = evaluate_metric(model, test_iter, scaler)\n",
        "    return l, MAE, MAPE, RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment driver\n",
        "\n",
        "This is to run experiments with different values for hyperparameters, and store the results in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A total number of 1 learning tasks will be executed 1 times\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "\n",
        "# The type of adjacency matrix\n",
        "options['adjacency_matrix'] = [\"Reciprocal\"] # [\"Raw\", \"Random\", \"Constant\", \"Reciprocal\", \"Exponential reciprocal\", \"Laplacian\"]\n",
        "# The degree of kernels\n",
        "options['spatial_kernel_size'] = [4]\n",
        "options['temporal_kernel_size'] = [3]\n",
        "# The number of epochs\n",
        "options['n_epochs'] = [20]\n",
        "# The (initial) learning rate\n",
        "options['learning_rate'] = [0.01]\n",
        "# History length (the number of consecutive timestep from which the model learns)\n",
        "options['history_length'] = [10]\n",
        "# The size (in years or timesteps) of train, validation and test sets\n",
        "options['splits_size'] = [(35,12,13)]\n",
        "\n",
        "number_of_runs = 1 # This is used to run the same test several times\n",
        "\n",
        "n = 0\n",
        "for _ in options['adjacency_matrix']:\n",
        "    for _ in options['spatial_kernel_size']:\n",
        "        for _ in options['temporal_kernel_size']:\n",
        "            for _ in options['n_epochs']:\n",
        "                for _ in options['learning_rate']:\n",
        "                    for _ in options['history_length']:\n",
        "                        for _ in options['splits_size']:\n",
        "                            n = n+1\n",
        "print(f'A total number of {n} learning tasks will be executed {number_of_runs} times')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the reciprocal\n",
            "\n",
            "STARTING TRAINING\n",
            "\n",
            "Training for 20 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/damiano/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:4552: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:472.)\n",
            "  return torch._C._nn.pad(input, pad, mode, value)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list assignment index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSTARTING TRAINING\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 31\u001b[0m min_train_loss, best_epoch_train, min_val_loss, best_epoch_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m train_time \u001b[38;5;241m=\u001b[39m end_time\u001b[38;5;241m-\u001b[39mstart_time\n",
            "Cell \u001b[0;32mIn[15], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, train_iter, val_iter)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m patience:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Keeping the last validation losses\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(patience)):\n\u001b[0;32m---> 51\u001b[0m         \u001b[43mlast_val_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m last_val_losses[i]\n\u001b[1;32m     52\u001b[0m     last_val_losses[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m val_loss\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(min_val_loss \u001b[38;5;241m<\u001b[39m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m last_val_losses):\n",
            "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
          ]
        }
      ],
      "source": [
        "n = 0\n",
        "\n",
        "this_run = 0\n",
        "while (this_run<number_of_runs):\n",
        "    for adjacency_matrix in options['adjacency_matrix']:\n",
        "        for spatial_kernel_size in options['spatial_kernel_size']:\n",
        "            for temporal_kernel_size in options['temporal_kernel_size']:\n",
        "                for n_epochs in options['n_epochs']:\n",
        "                    for learning_rate in options['learning_rate']:\n",
        "                        for history_length in options['history_length']:\n",
        "                            # TO-DO: This is unused for the moment (train_iter, val_iter and test_iter have been computed before)\n",
        "                            for splits_size in options['splits_size']:\n",
        "                                # TO-DO: improve this by only specifying the kind of distance we want (to indicate the path should be redundant)\n",
        "                                A = dist_matrix.get_adjacency_matrix(adjacency_matrix,dist_matrix_dict,\"Kms\",spatial_kernel_size)\n",
        "                                A = A.to(device)\n",
        "                                train_iter, val_iter, test_iter = get_input_data(splits_size[0], splits_size[1], splits_size[2], history_length)\n",
        "                                n_features = A.shape[1]\n",
        "                                # Parameters:\n",
        "                                # - ks: Size of the spatial kernel\n",
        "                                # - kt: Size of the temporal kernel\n",
        "                                # - bs: Lists of channels for each block\n",
        "                                # - T: Number of timesteps\n",
        "                                # - n: Number of time intervals (slots)\n",
        "                                # - Lk: Chebyshev polynomials\n",
        "                                # - p: Dropout probability\n",
        "                                model = STGCN(spatial_kernel_size, temporal_kernel_size, blocks, history_length, n_features, A, drop_prob).to(device)\n",
        "                                optimizer = get_optimizer(\"RMSprop\",model,learning_rate)\n",
        "                                scheduler = get_scheduler(\"CosineAnnealingLR\",optimizer,n_epochs)\n",
        "                                print(\"\\nSTARTING TRAINING\\n\")\n",
        "                                start_time = time.time()\n",
        "                                min_train_loss, best_epoch_train, min_val_loss, best_epoch_val = train(model,n_epochs,train_iter,val_iter)\n",
        "                                end_time = time.time()\n",
        "                                train_time = end_time-start_time\n",
        "                                print(f\"Best Train Loss: {min_train_loss:.4f} at epoch {best_epoch_train}\")\n",
        "                                print(f\"Best Validation Loss: {min_val_loss:.4f} at epoch {best_epoch_val}\")\n",
        "                                print(f\"Training time: {train_time} s\")\n",
        "                                print(\"\\nSTARTING TESTING\\n\")\n",
        "                                test_loss, MAE, MAPE, RMSE = test(model,test_iter)\n",
        "                                print( f\"Test loss: {test_loss:.4f}\")\n",
        "                                print( f\"MAE: {MAE:.4f}, MAPE: {MAPE:.2f}%, RMSE: {RMSE:.4f}\")\n",
        "                                output = {\n",
        "                                    # params\n",
        "                                    \"adjacency_matrix\": adjacency_matrix,\n",
        "                                    \"spatial_kernel_size\": spatial_kernel_size,\n",
        "                                    \"temporal_kernel_size\": temporal_kernel_size,\n",
        "                                    \"n_epochs\": n_epochs,\n",
        "                                    \"learning_rate\": learning_rate,\n",
        "                                    \"history_length\": history_length,\n",
        "                                    \"train_size\": splits_size[0],\n",
        "                                    \"val_size\": splits_size[1],\n",
        "                                    \"test_size\": splits_size[2],\n",
        "                                    # results\n",
        "                                    \"min_train_loss\": min_train_loss,\n",
        "                                    \"best_epoch_train\": best_epoch_train,\n",
        "                                    \"min_val_loss\": min_val_loss,\n",
        "                                    \"best_epoch_val\": best_epoch_val,\n",
        "                                    \"train_time\": train_time,\n",
        "                                    \"test_loss\": test_loss,\n",
        "                                    \"MAE\": MAE,\n",
        "                                    \"MAPE\": MAPE,\n",
        "                                    \"RMSE\": RMSE\n",
        "                                } \n",
        "                                write_line_to_xlsx(output)\n",
        "                                print(f\"### Learning task #{n+1} done\\n\\n\")\n",
        "                                n = n+1\n",
        "    this_run = this_run+1\n",
        "print(f'Executed {n} learning tasks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# END OF MY CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwbqhU3QVxxp"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP4n8UIgVxxp"
      },
      "source": [
        "#### Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDmYbf2NQsUI"
      },
      "outputs": [],
      "source": [
        "def filter_lists(list1, list2, limit, min_index=0, max_index=100 ):\n",
        "    list1 = list1.copy()\n",
        "    list2 = list2.copy()\n",
        "\n",
        "    indices_to_be_removed = []\n",
        "\n",
        "    # Identifying the indices to be removed after the first pass\n",
        "    for index, (value1, value2) in enumerate(zip(list1, list2)):\n",
        "        if value1 >= limit or value2 >= limit:\n",
        "            indices_to_be_removed.append(index)\n",
        "\n",
        "    # Removing indices in reversed order\n",
        "    for index in sorted(indices_to_be_removed, reverse=True):\n",
        "        list1.pop(index)\n",
        "        list2.pop(index)\n",
        "\n",
        "    # Removing all elements after max_index\n",
        "    if max_index < len(list1):\n",
        "        del list1[max_index+1:]\n",
        "        del list2[max_index+1:]\n",
        "\n",
        "    # Removing all elements before min_index\n",
        "    if min_index > 0:\n",
        "        list1 = list1[min_index:]\n",
        "        list2 = list2[min_index:]\n",
        "\n",
        "    return list1, list2\n",
        "\n",
        "def filter_list(list, limit, min_index=0, max_index=100):\n",
        "    list = list.copy()\n",
        "\n",
        "    indices_to_be_removed = []\n",
        "\n",
        "    # Identifying the indices to be removed after the first pass\n",
        "    for index, value in enumerate(list):\n",
        "        if value >= limit:\n",
        "            indices_to_be_removed.append(index)\n",
        "\n",
        "    # Removing indices in reversed order\n",
        "    for index in sorted(indices_to_be_removed, reverse=True):\n",
        "        list.pop(index)\n",
        "\n",
        "    # Removing all elements after max_index\n",
        "    if max_index < len(list):\n",
        "        del list[max_index+1:]\n",
        "\n",
        "    # Removing all elements before min_index\n",
        "    if min_index > 0:\n",
        "        list = list[min_index:]\n",
        "\n",
        "    return list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NIld0T6Vxxq"
      },
      "source": [
        "#### Showing losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "lcGRplUs3gn7",
        "outputId": "173b0aed-c347-4889-bf8a-52e2eeb28176"
      },
      "outputs": [],
      "source": [
        "# This is to show only part of the results in a chart\n",
        "\n",
        "# Parameter configuration for loss filtering\n",
        "# 'filter' indicates whether filters have to be applied to the lists of losses\n",
        "filter = True\n",
        "# Parameters specify the epoch range and the maximum value for a loss to be kept\n",
        "max_value = 50\n",
        "min_epoch = 60\n",
        "max_epoch = 200\n",
        "\n",
        "if filter:\n",
        "    # Applying the filter to obtain loss lists that fit the constraints\n",
        "    train_loss_list_fil, val_loss_list_fil = filter_lists(train_loss_list, val_loss_list, max_value, min_epoch, max_epoch)\n",
        "    epochs_range = range(min_epoch + 1, min_epoch + 1 + len(train_loss_list_fil))\n",
        "else:\n",
        "    # If filtering is off, use the original loss lists\n",
        "    train_loss_list_fil, val_loss_list_fil = train_loss_list, val_loss_list\n",
        "    epochs_range = range(1, len(train_loss_list_fil) + 1)\n",
        "\n",
        "# Creating the graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs_range, train_loss_list_fil, label='Train Loss', marker='o')\n",
        "plt.plot(epochs_range, val_loss_list_fil, label='Validation Loss', marker='o')\n",
        "# Adding title and labels\n",
        "plt.title('Train and Validation Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T36na-SMVxxq"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y1laXXWVxxq",
        "outputId": "766c3bea-6e8d-4e90-9823-74103c73c7c0"
      },
      "outputs": [],
      "source": [
        "if (drive):\n",
        "    save_model_path = f'/content/drive/My Drive/TFM/models/model-{generate_id()[:12]}.pth'\n",
        "    results_path = f'/content/drive/My Drive/TFM/models/model-{generate_id()[:12]}.pth'\n",
        "else:\n",
        "    save_model_path = os.path.join(main_dir, \"models\", f'model-{generate_id()[:12]}.pth')\n",
        "    results_path = os.path.join(main_dir, \"results\", f'results-{generate_id()[:12]}.json')\n",
        "\n",
        "print(save_model_path)\n",
        "print(results_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPyUv31DVxxq"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), save_model_path)\n",
        "\n",
        "# Creating a dictionary with lists\n",
        "data = {\n",
        "    'train_loss_list': train_loss_list,\n",
        "    'val_loss_list': val_loss_list,\n",
        "    'MAE': f\"{MAE:.4f}\",\n",
        "    'MAPE': f\"{MAPE:.2f}%\",\n",
        "    'RMSE': f\"{RMSE:.4f}\",\n",
        "}\n",
        "\n",
        "# Save to a JSON file\n",
        "with open(results_path, 'w') as json_file:\n",
        "    json.dump(data, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEkcLMedO41s"
      },
      "source": [
        "# Load Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd4Ih-P-Vxxp",
        "outputId": "39166510-6abc-4ffd-c07e-869f08ff38cd"
      },
      "outputs": [],
      "source": [
        "model_id = 202407032036\n",
        "# 202407012024\n",
        "\n",
        "drive = False\n",
        "if (drive):\n",
        "    load_path = f'/content/drive/My Drive/TFM/models/model-{model_id}.pth'\n",
        "else:\n",
        "    load_path = os.path.join(main_dir, \"models\", f'model-{model_id}.pth')\n",
        "\n",
        "print(load_path)\n",
        "\n",
        "model = STGCN(Ks, Kt, blocks, n_his, n_features, Lk, drop_prob).to(device)\n",
        "state_dict = torch.load(load_path)\n",
        "model.load_state_dict(state_dict)\n",
        "type(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89OYVdJtgfS"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inovirxTyQAN"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# (back-propagation is not needed here)\n",
        "with torch.no_grad():\n",
        "    # Inicializing lists to store error metrics\n",
        "    mae, mape, mse = [], [], []\n",
        "\n",
        "    # Iterating on training data\n",
        "    for x, y in test_iter:\n",
        "        x_raw = x # Store original input data\n",
        "        y_raw = y # Store original labels\n",
        "\n",
        "        # Transforming ground labels to their original scale\n",
        "        y = scaler.inverse_transform(y.cpu().numpy()).reshape(-1)\n",
        "\n",
        "        # Running model prediction and transforming to original scale\n",
        "        y_pred = scaler.inverse_transform(model(x).view(len(x), -1).cpu().numpy()).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlGmLU6ZApdI"
      },
      "outputs": [],
      "source": [
        "# Returns the index and column where the value is found\n",
        "def search_value_in_dataframe(df, value):\n",
        "    result = []  \n",
        "\n",
        "    for col in df.columns:\n",
        "        matching_rows = df[df[col] == value].index.tolist()\n",
        "\n",
        "        for row in matching_rows:\n",
        "            result.append((row, col))\n",
        "\n",
        "    return result  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "BBolT1bOyzSL",
        "outputId": "74b90146-6e86-42ec-f3bf-f57ee85447f8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ground and predicted values\n",
        "data1 = y\n",
        "data2 = y_pred\n",
        "\n",
        "# Creating a figure\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Showing the first data set\n",
        "plt.plot(data1, label='Data 1', color='b')\n",
        "\n",
        "# Showing the second data set over the first\n",
        "plt.plot(data2, label='Data 2', color='r')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Overlapping of two data sets')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quQLGhptn1sW",
        "outputId": "592c222f-8589-4fe5-ebc1-5c7f16d01ee3"
      },
      "outputs": [],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y8v4GBRpzIj"
      },
      "source": [
        "### Predictions for 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "nV1t1j54nsV-",
        "outputId": "83c69365-1adc-470f-d0ad-b58c2a4d4845"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Selecting a year (131 countries, columns) for input data and predictions\n",
        "data1 = y[:130] \n",
        "data2 = y_pred[:130]  \n",
        "\n",
        "# Creating a figure\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Defining the index for bars\n",
        "indices = np.arange(len(data1))\n",
        "\n",
        "# Bar width\n",
        "width = 0.4\n",
        "\n",
        "# Show the first data set as bars\n",
        "plt.bar(indices - width/2, data1, width=width, label='Data 1', color='blue')\n",
        "\n",
        "# Show the first data set as bars\n",
        "plt.bar(indices + width/2, data2, width=width, label='Data 2', color='red')\n",
        "\n",
        "plt.title('Comparing two data sets')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGRAhxfPoT-U",
        "outputId": "294dc02a-0447-42cf-8e77-b3b8d6f3d8eb"
      },
      "outputs": [],
      "source": [
        "list(matrix_df.columns)[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52O6ard2peMT",
        "outputId": "7c920bde-5c5b-46e2-e9ed-3d2ce202cb97"
      },
      "outputs": [],
      "source": [
        "list(matrix_df.columns)[61]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEZZyvAcoRBW",
        "outputId": "83ef0c90-eecc-437c-eac7-5593fc32ee7c"
      },
      "outputs": [],
      "source": [
        "# Calcular la diferencia absoluta entre los valores correspondientes\n",
        "diferencias = np.abs(data1[:130] - data2[:130])\n",
        "\n",
        "# Encontrar la posicin con la mayor diferencia\n",
        "posicion_max_diferencia = np.argmax(diferencias)\n",
        "mayor_diferencia = diferencias[posicion_max_diferencia]\n",
        "\n",
        "print(f\"La posicin con la mayor diferencia es: {posicion_max_diferencia}\")\n",
        "print(f\"La mayor diferencia es: {mayor_diferencia}\")\n",
        "varianza = np.var(data_df['ALB'])\n",
        "varianza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "sKolOG79ojux",
        "outputId": "da46f3d7-7a18-4b77-e841-7849e34af1f4"
      },
      "outputs": [],
      "source": [
        "type(data_df['AUS'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data_df['AUS'], label='Series Data')\n",
        "plt.xlabel('ndice')\n",
        "plt.ylabel('Valor')\n",
        "plt.title('Grfica de pandas.core.series.Series')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYcqvkeApWZH",
        "outputId": "e60d8c9b-8590-41a9-c6bc-e344d982a83e"
      },
      "outputs": [],
      "source": [
        "# Calcular la diferencia absoluta entre los valores correspondientes\n",
        "diferencias = np.abs(data1 - data2)\n",
        "\n",
        "# Encontrar la posicin con la mayor diferencia\n",
        "posicion_max_diferencia = np.argmax(diferencias)\n",
        "mayor_diferencia = diferencias[posicion_max_diferencia]\n",
        "\n",
        "print(f\"El pais con la mayor diferencia es: {list(matrix_df.columns)[posicion_max_diferencia]}\")\n",
        "print(f\"La mayor diferencia es: {mayor_diferencia}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "P_88dxoxpbO6",
        "outputId": "7181532f-fc75-4119-800b-f8e7fa192c6c"
      },
      "outputs": [],
      "source": [
        "type(data_df['IRQ'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data_df['IRQ'], label='Series Data')\n",
        "plt.xlabel('ndice')\n",
        "plt.ylabel('Valor')\n",
        "plt.title('Grfica de pandas.core.series.Series')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LV03WFAGwZ1"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-UujPPRGY5w"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfUoO_WnGtoI"
      },
      "outputs": [],
      "source": [
        "y_1 = y[:131]\n",
        "y_2 = y[131:]\n",
        "\n",
        "y_pred_1 = y_pred[:131]\n",
        "y_pred_2 = y_pred[:131]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlhUTSK3GkGG"
      },
      "outputs": [],
      "source": [
        "results_2020 = pd.DataFrame([y_1, y_pred_1], columns=data.columns)\n",
        "results_2021 = pd.DataFrame([y_2, y_pred_2], columns=data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "Y-uROuaCIjAe",
        "outputId": "6bedb074-45ff-45ac-afaf-9989fd69d84d"
      },
      "outputs": [],
      "source": [
        "fila_1 = results_2020.iloc[0]  # Primera fila\n",
        "fila_2 = results_2020.iloc[1]  # Segunda fila\n",
        "\n",
        "# Configurar los valores de las dos filas\n",
        "valores_fila_1 = fila_1.values\n",
        "valores_fila_2 = fila_2.values\n",
        "\n",
        "labels = fila_1.index\n",
        "\n",
        "# Crear la grfica de barras\n",
        "x = range(len(labels))\n",
        "width = 0.4  # Ancho de las barras\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Ajustar el tamao de la figura si es necesario\n",
        "ax.bar(x, valores_fila_1, width=width, label='Fila 1', align='center')\n",
        "ax.bar([p + width for p in x], valores_fila_2, width=width, label='Fila 2', align='center')\n",
        "\n",
        "# Aadir etiquetas y ttulo\n",
        "ax.set_xlabel('Columnas')\n",
        "ax.set_ylabel('Valores')\n",
        "ax.set_title('Comparacin de Valores entre dos Filas')\n",
        "ax.set_xticks([p + width / 2 for p in x])\n",
        "ax.set_xticklabels([''] * len(labels))  # Configurar las etiquetas del eje x como vacas\n",
        "ax.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnA5y6R2p4LN"
      },
      "source": [
        "### predicts 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "gjT1xIFLp4LN",
        "outputId": "c22791a2-53aa-4d4a-99de-6f32d8519424"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supongamos que tienes dos numpy arrays de ejemplo\n",
        "data1 = y[131:]  # Ejemplo de datos aleatorios\n",
        "data2 = y_pred[131:]  # Ejemplo de datos aleatorios\n",
        "\n",
        "# Crear una figura y un conjunto de ejes\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Definir el ndice para las barras\n",
        "indices = np.arange(len(data1))\n",
        "\n",
        "# Ancho de las barras\n",
        "width = 0.4\n",
        "\n",
        "# Graficar el primer conjunto de datos como barras\n",
        "plt.bar(indices - width/2, data1, width=width, label='Data 1', color='b')\n",
        "\n",
        "# Graficar el segundo conjunto de datos como barras\n",
        "plt.bar(indices + width/2, data2, width=width, label='Data 2', color='r')\n",
        "\n",
        "# Aadir ttulo y etiquetas\n",
        "plt.title('Comparacin de dos conjuntos de datos')\n",
        "plt.xlabel('ndice')\n",
        "plt.ylabel('Valor')\n",
        "\n",
        "# Aadir una leyenda\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar el grfico\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io05StIsp4LO",
        "outputId": "2b617002-1b83-4e77-e2e8-4b3c697199ff"
      },
      "outputs": [],
      "source": [
        "list(matrix_df.columns)[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYqHKTvHp4LO"
      },
      "outputs": [],
      "source": [
        "data1 = y[131:]\n",
        "data2 = y_pred[131:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qqp_JTDp4LP",
        "outputId": "e9784f4a-50a8-439a-fcda-473bc41508e0"
      },
      "outputs": [],
      "source": [
        "# Calcular la diferencia absoluta entre los valores correspondientes\n",
        "diferencias = np.abs(data1 - data2)\n",
        "\n",
        "# Encontrar la posicin con la mayor diferencia\n",
        "posicion_max_diferencia = np.argmax(diferencias)\n",
        "mayor_diferencia = diferencias[posicion_max_diferencia]\n",
        "\n",
        "print(f\"El pais con la mayor diferencia es: {list(matrix_df.columns)[posicion_max_diferencia]}\")\n",
        "print(f\"La mayor diferencia es: {mayor_diferencia}\")\n",
        "print( f\"Varianza de {list(matrix_df.columns)[posicion_max_diferencia]} : {np.var(data_df[list(matrix_df.columns)[posicion_max_diferencia]])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PJIVMJqn8i",
        "outputId": "396b256e-f587-4d46-96c6-856a22a0e8a5"
      },
      "outputs": [],
      "source": [
        "varianza = np.var(data_df[list(matrix_df.columns)[posicion_max_diferencia]])\n",
        "print( f\"Varianza de {list(matrix_df.columns)[posicion_max_diferencia]} : {}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "m3nN3pDLp4LP",
        "outputId": "83c5e322-f9a0-4cfa-aa57-4971bb7f3487"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "plt.plot(data_df['ALB'], label='Series Data')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('SITCECI')\n",
        "plt.title('SITCECI of Albania')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4vGHumBp4LP",
        "outputId": "70466538-0cc8-4c38-e1ad-75598be33627"
      },
      "outputs": [],
      "source": [
        "# Calcular la diferencia absoluta entre los valores correspondientes\n",
        "diferencias = np.abs(data1 - data2)\n",
        "\n",
        "# Encontrar la posicin con la mayor diferencia\n",
        "posicion_max_diferencia = np.argmin(diferencias)\n",
        "mayor_diferencia = diferencias[posicion_max_diferencia]\n",
        "\n",
        "print(f\"El pais con la menor diferencia es: {list(matrix_df.columns)[posicion_max_diferencia]}\")\n",
        "print(f\"La menor diferencia es: {mayor_diferencia}\")\n",
        "print( f\"Varianza de {list(matrix_df.columns)[posicion_max_diferencia]} : {np.var(data_df[list(matrix_df.columns)[posicion_max_diferencia]])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "KmEDm0Okp4LP",
        "outputId": "112ec580-4ece-44f8-8390-3d8f83ea6a28"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "plt.plot(data_df['PHL'], label='Series Data')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('SITCECI')\n",
        "plt.title('SITCECI of The Philippines')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YsWjW9Vp4LQ"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f71OgtxHp4LR"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr5F6kPop4LR"
      },
      "outputs": [],
      "source": [
        "y_1 = y[:131]\n",
        "y_2 = y[131:]\n",
        "\n",
        "y_pred_1 = y_pred[:131]\n",
        "y_pred_2 = y_pred[:131]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GbQn0Sfp4LR"
      },
      "outputs": [],
      "source": [
        "results_2020 = pd.DataFrame([y_1, y_pred_1], columns=data.columns)\n",
        "results_2021 = pd.DataFrame([y_2, y_pred_2], columns=data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "IoP0nJ89p4LR",
        "outputId": "b06d2823-783b-45e0-d694-ae43f3c3946b"
      },
      "outputs": [],
      "source": [
        "fila_1 = results_2020.iloc[0]  # Primera fila\n",
        "fila_2 = results_2020.iloc[1]  # Segunda fila\n",
        "\n",
        "# Configurar los valores de las dos filas\n",
        "valores_fila_1 = fila_1.values\n",
        "valores_fila_2 = fila_2.values\n",
        "\n",
        "labels = fila_1.index\n",
        "\n",
        "# Crear la grfica de barras\n",
        "x = range(len(labels))\n",
        "width = 0.4  # Ancho de las barras\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Ajustar el tamao de la figura si es necesario\n",
        "ax.bar(x, valores_fila_1, width=width, label='Fila 1', align='center')\n",
        "ax.bar([p + width for p in x], valores_fila_2, width=width, label='Fila 2', align='center')\n",
        "\n",
        "# Aadir etiquetas y ttulo\n",
        "ax.set_xlabel('Columnas')\n",
        "ax.set_ylabel('Valores')\n",
        "ax.set_title('Comparacin de Valores entre dos Filas')\n",
        "ax.set_xticks([p + width / 2 for p in x])\n",
        "ax.set_xticklabels([''] * len(labels))  # Configurar las etiquetas del eje x como vacas\n",
        "ax.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYOZs6aquQ6d"
      },
      "source": [
        "### countries study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1GkgZtiwDX_"
      },
      "outputs": [],
      "source": [
        "matrix_columns = list(matrix_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1nNcEkluT6D"
      },
      "outputs": [],
      "source": [
        "def n_closest_countries(df, country, N):\n",
        "    distances = df[country]\n",
        "    sorted_distances = distances.sort_values()\n",
        "    closest_indices = sorted_distances.index[1:N+1]  # Skip the first one as it is the country itself\n",
        "    closest_countries = [matrix_columns[i] for i in closest_indices]\n",
        "    return closest_countries\n",
        "\n",
        "def n_farthest_countries(df, country, N):\n",
        "    distances = df[country]\n",
        "    sorted_distances = distances.sort_values(ascending=False)\n",
        "    farthest_indices = sorted_distances.index[:N]\n",
        "    farthest_countries = [matrix_columns[i] for i in farthest_indices]\n",
        "    return farthest_countries\n",
        "\n",
        "def calcular_varianza(df, columna, previous_n=60):\n",
        "    varianza = df[columna][:previous_n].var()\n",
        "    return varianza\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL_hKXUZw9vc"
      },
      "outputs": [],
      "source": [
        "def get_data_countries(country_name, N=5):\n",
        "    # Calcular la varianza para cada uno de los pases ms cercanos y calcular la media de estas varianzas\n",
        "    closest_countries = n_closest_countries(matrix_df, country_name, N)\n",
        "    varianzas = [calcular_varianza(data_df, country, 10) for country in closest_countries]\n",
        "    media_varianzas = np.mean(varianzas)\n",
        "\n",
        "    print(f\"Los {N} pases ms cercanos a {country_name} son: {closest_countries}\")\n",
        "    print(f\"La varianza de cada uno de estos pase de los ultimos 10 aos es: {[round(v, 4) for v in varianzas]}\")\n",
        "    print(f\"La media de las varianzas es: {round(media_varianzas, 4)}\")\n",
        "\n",
        "    # Calcular la varianza para cada uno de los pases ms cercanos y calcular la media de estas varianzas\n",
        "    farthest_countries = n_farthest_countries(matrix_df, country_name, N)\n",
        "    varianzas = [calcular_varianza(data_df, country, 10) for country in farthest_countries]\n",
        "    media_varianzas = np.mean(varianzas)\n",
        "\n",
        "    print(f\"Los {N} pases ms lejanos a {country_name} son: {farthest_countries}\")\n",
        "    print(f\"La varianza de cada uno de estos pases de los ultimos 10 aos es: {[round(v, 4) for v in varianzas]}\")\n",
        "    print(f\"La media de las varianzas es: {round(media_varianzas, 4)}\")\n",
        "\n",
        "    return closest_countries, farthest_countries\n",
        "\n",
        "def represent_countries(countries, previous_n=10):\n",
        "  # Filtrar las columnas ms cercanas\n",
        "  filtered_df = data_df[countries][:previous_n]\n",
        "\n",
        "  # Crear la grfica\n",
        "  plt.figure(figsize=(14, 8))\n",
        "\n",
        "  # Graficar cada columna\n",
        "  for column in filtered_df.columns:\n",
        "      plt.plot(filtered_df.index, filtered_df[column], label=column)\n",
        "\n",
        "  # Aadir ttulo y etiquetas\n",
        "  plt.title('Valores de las columnas ms cercanas')\n",
        "  plt.xlabel('ndice')\n",
        "  plt.ylabel('Valor')\n",
        "\n",
        "  # Aadir una leyenda\n",
        "  plt.legend()\n",
        "\n",
        "  # Mostrar la grfica\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fP-jGIxVSO",
        "outputId": "71447310-8aff-4827-ba49-b98276a5277a"
      },
      "outputs": [],
      "source": [
        "closest_countries, farthest_countries = get_data_countries('ALB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "lKon6oqq3tur",
        "outputId": "32e75da5-f323-4e48-8f62-366683cf674e"
      },
      "outputs": [],
      "source": [
        "represent_countries(farthest_countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwDDbl7jgB_"
      },
      "source": [
        "# Comparacin modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCQNDbeNjjBV"
      },
      "source": [
        "## Load results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9fiNRUwjvkY"
      },
      "outputs": [],
      "source": [
        "regular_data = \"results-20240630\"\n",
        "inverted_matrix = \"results-20240701\"\n",
        "\n",
        "results_main_path = '/mnt/e/ARCHIVOS/UNIVERSIDAD/MUIA/TFM/Development/stgcn-colab/results'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1dJOZbnmdlm"
      },
      "source": [
        "### List files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgP4Qe0MkIjp",
        "outputId": "e209ea0f-66c1-4ac9-e08c-d4eb7acfefa5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def list_files(carpeta, prefijo):\n",
        "    # Lista para almacenar los paths de los archivos que cumplen con el criterio\n",
        "    archivos_filtrados = []\n",
        "\n",
        "    # Iterar sobre los archivos en la carpeta\n",
        "    for archivo in os.listdir(carpeta):\n",
        "        # Comprobar si el archivo comienza con el prefijo especificado\n",
        "        if archivo.startswith(prefijo):\n",
        "            # Crear el path completo del archivo y aadirlo a la lista\n",
        "            path_completo = os.path.join(carpeta, archivo)\n",
        "            archivos_filtrados.append(path_completo)\n",
        "\n",
        "    return archivos_filtrados\n",
        "\n",
        "result_list = list_files(results_main_path, regular_data)\n",
        "\n",
        "# Imprimir los paths de los archivos que cumplen con el criterio\n",
        "for result_file in result_list:\n",
        "    print(result_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx15DDWMll_4",
        "outputId": "4ce34d0f-307b-4e7d-f444-8feec0d041c0"
      },
      "outputs": [],
      "source": [
        "results_dict = {\n",
        "    \"AdamW - CosineAnnealingLR\": result_list[0],\n",
        "    \"RMSprop - CosineAnnealingLR\": result_list[1],\n",
        "    \"AdamW -CosineAnnealingLR\": result_list[2],\n",
        "    \"AdamW -CosineAnnealingLR\": result_list[3],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkMQh5ymgwc"
      },
      "source": [
        "### Enum files and search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoHGe01Kqfje"
      },
      "outputs": [],
      "source": [
        "def leer_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def leer_result(file_path):\n",
        "    file_path = f'{results_main_path}/results-{file_path}.json'\n",
        "    return leer_json(file_path)\n",
        "\n",
        "def get_results_data(model_dict):\n",
        "    for key, value in model_dict.items():\n",
        "        # Leer el archivo JSON correspondiente usando el \"id\"\n",
        "        json_data = leer_result(value[\"id\"])\n",
        "\n",
        "        # Aadir los datos del JSON al diccionario bajo la misma clave\n",
        "        model_dict[key].update(json_data)\n",
        "\n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbVnIg2umiqU"
      },
      "outputs": [],
      "source": [
        "regular_data_dict = {\n",
        "    0: {\"name\": \"AdamW - CosineAnnealingLR\", \"id\": 202407032033},\n",
        "    1: {\"name\": \"RMSprop - CosineAnnealingLR\", \"id\": 202407042014},\n",
        "    2: {\"name\": \"RMSprop - StepLR\", \"id\": 202407032036},\n",
        "    3: {\"name\": \"AdamW - StepLR\", \"id\": 202407032034},\n",
        "}\n",
        "\n",
        "inverted_matrix_dict = {\n",
        "    1: {\"name\": \"RMSprop - CosineAnnealingLR\", \"id\": 202407032058},\n",
        "    2: {\"name\": \"RMSprop - StepLR\", \"id\": 202407032056},\n",
        "}\n",
        "\n",
        "exp_inverted_matrix_dict = {\n",
        "    1: {\"name\": \"RMSprop - CosineAnnealingLR\", \"id\": 202407032106},\n",
        "    2: {\"name\": \"RMSprop - StepLR\", \"id\": 202407032107},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbgAqjd3v2SL"
      },
      "source": [
        "## Grafico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByhYE1yMqq5N"
      },
      "outputs": [],
      "source": [
        "results_data = get_results_data(regular_data_dict)\n",
        "\n",
        "regular_result_loss = leer_result('202407042014')['val_loss_list']\n",
        "inverted_result_loss = leer_result('202407032058')['val_loss_list']\n",
        "exp_inverted_result_loss = leer_result('202407032106')['val_loss_list']\n",
        "exp_inverted_loss_expanded = leer_result('202407042053')['val_loss_list']\n",
        "exp_inverted_loss_expanded_v2 = leer_result('202407042053')['val_loss_list']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1QlOG6DOma"
      },
      "source": [
        "### Listas de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "9x28wtrktyk_",
        "outputId": "d15eb0c4-fc5f-44a9-df1a-45c622566d79"
      },
      "outputs": [],
      "source": [
        "# Filtrar\n",
        "    # True | False\n",
        "filtrar = True\n",
        "max_value = 3.5\n",
        "min_epoch = 60\n",
        "max_epoch = 200\n",
        "# 'train_loss_list' | 'val_loss_list'\n",
        "loss_list_name = 'val_loss_list'\n",
        "\n",
        "# Crear la grfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for key, value in results_data.items():\n",
        "    loss_list = value[loss_list_name]\n",
        "\n",
        "    if(filtrar):\n",
        "        loss_list_fil = filtrar_valores_lista(loss_list, max_value, min_epoch, max_epoch)\n",
        "        epochs_range = range(min_epoch + 1, min_epoch + 1 + len(loss_list_fil))\n",
        "    else:\n",
        "        loss_list_fil = loss_list\n",
        "        epochs_range = range(1, len(loss_list_fil) + 1)\n",
        "\n",
        "    name = value['name']\n",
        "    plt.plot(epochs_range, loss_list_fil, label=name, marker='o')\n",
        "\n",
        "# Aadir ttulos y etiquetas\n",
        "plt.title('Val Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKsNl1SiDu-y"
      },
      "source": [
        "### Mtricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-YRogAnSD-Js",
        "outputId": "2fba6e1f-ee48-44ce-ebdd-dd6c509d5044"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Recopilar los valores de MAPE\n",
        "model_names = []\n",
        "mape_values = []\n",
        "\n",
        "for key, value in results_data.items():\n",
        "    model_names.append(value['name'])\n",
        "    mape_values.append(float(value['MAPE'].strip('%')))\n",
        "\n",
        "# Configurar la posicin de las barras\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.4  # Ancho de las barras\n",
        "\n",
        "# Crear la grfica\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Aadir las barras para MAPE\n",
        "rects = ax.bar(x, mape_values, width, label='MAPE')\n",
        "\n",
        "# Aadir etiquetas y ttulos\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('MAPE (%)')\n",
        "ax.set_title('Model MAPE Performance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "# Ajustar el lmite superior del eje y para dar ms margen\n",
        "max_height = max(mape_values)\n",
        "ax.set_ylim(0, max_height + 5)  # Aade un margen del 5% sobre el valor mximo\n",
        "\n",
        "# Aadir los valores encima de las barras\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(round(height, 2)),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects)\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3EMgIFSCDeMi",
        "outputId": "fe93436e-d4cc-4f30-c61e-d0de7e578d26"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Recopilar los valores de MAE, MAPE y RMSE\n",
        "model_names = []\n",
        "mae_values = []\n",
        "mape_values = []\n",
        "rmse_values = []\n",
        "\n",
        "for key, value in results_data.items():\n",
        "    model_names.append(value['name'])\n",
        "    mae_values.append(float(value['MAE']))\n",
        "    rmse_values.append(float(value['RMSE']))\n",
        "\n",
        "# Configurar la posicin de las barras\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2  # Ancho de las barras\n",
        "\n",
        "# Crear la grfica\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Aadir las barras para MAE, MAPE y RMSE\n",
        "rects1 = ax.bar(x - width, mae_values, width, label='MAE')\n",
        "rects3 = ax.bar(x + width, rmse_values, width, label='RMSE')\n",
        "\n",
        "# Aadir etiquetas y ttulos\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Performance Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "# Aadir los valores encima de las barras\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(round(height, 5)),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 5),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects3)\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3BPFC-PTtPq"
      },
      "source": [
        "### All data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "kMgTb9OuTvfE",
        "outputId": "76e43745-a658-49f9-854d-82d6274ca472"
      },
      "outputs": [],
      "source": [
        "# Parmetros de filtrado\n",
        "# False | True\n",
        "filtrar = True\n",
        "max_value = 1000\n",
        "min_epoch = 10\n",
        "max_epoch = 150\n",
        "\n",
        "# Filtrar las listas de resultados si es necesario\n",
        "if filtrar:\n",
        "    regular_result_loss_fil = filtrar_valores_lista(regular_result_loss, max_value, min_epoch, max_epoch)\n",
        "    inverted_result_loss_fil = filtrar_valores_lista(inverted_result_loss, max_value, min_epoch, max_epoch)\n",
        "    exp_inverted_result_loss_fil = filtrar_valores_lista(exp_inverted_result_loss, max_value, min_epoch, max_epoch)\n",
        "else:\n",
        "    regular_result_loss_fil = regular_result_loss\n",
        "    inverted_result_loss_fil = inverted_result_loss\n",
        "    exp_inverted_result_loss_fil = exp_inverted_result_loss\n",
        "\n",
        "# Crear rangos de pocas correspondientes\n",
        "epochs_range_regular = range(min_epoch + 1, min_epoch + 1 + len(regular_result_loss_fil))\n",
        "# epochs_range_inverted = range(min_epoch + 1, min_epoch + 1 + len(inverted_result_loss_fil))\n",
        "epochs_range_exp_inverted = range(min_epoch + 1, min_epoch + 1 + len(exp_inverted_result_loss_fil))\n",
        "\n",
        "# Crear la grfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plotear los resultados\n",
        "plt.plot(epochs_range_regular, regular_result_loss_fil, label='Regular Loss', marker='o')\n",
        "# plt.plot(epochs_range_inverted, inverted_result_loss_fil, label='Inverted Loss', marker='o')\n",
        "plt.plot(epochs_range_exp_inverted, exp_inverted_result_loss_fil, label='Exp Inverted Loss', marker='o')\n",
        "\n",
        "# Aadir ttulos y etiquetas\n",
        "plt.title('Validation Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHJmHX61e9mS"
      },
      "outputs": [],
      "source": [
        "exp_inverted_loss_expanded = exp_inverted_loss_expanded_fil\n",
        "exp_inverted_loss_expanded_v2 = val_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "VwV6zNVrGZD7",
        "outputId": "e899f99c-6b08-49ba-fafc-bd15eb96569c"
      },
      "outputs": [],
      "source": [
        "# False | True\n",
        "filtrar = True\n",
        "max_value = 300\n",
        "min_epoch = 0\n",
        "max_epoch = 10\n",
        "\n",
        "# Filtrar las listas de resultados si es necesario\n",
        "if filtrar:\n",
        "    regular_result_loss_fil = filtrar_valores_lista(regular_result_loss, max_value, min_epoch, max_epoch)\n",
        "    # inverted_result_loss_fil = filtrar_valores_lista(inverted_result_loss, max_value, min_epoch, max_epoch)\n",
        "    exp_inverted_result_loss_fil = filtrar_valores_lista(exp_inverted_result_loss, max_value, min_epoch, max_epoch)\n",
        "    exp_inverted_loss_expanded_fil = filtrar_valores_lista(exp_inverted_loss_expanded, max_value, min_epoch, max_epoch)\n",
        "    # exp_inverted_loss_expanded_v2_fil = filtrar_valores_lista(exp_inverted_loss_expanded_v2, max_value, min_epoch, max_epoch)\n",
        "else:\n",
        "    regular_result_loss_fil = regular_result_loss\n",
        "    # inverted_result_loss_fil = inverted_result_loss\n",
        "    exp_inverted_result_loss_fil = exp_inverted_result_loss\n",
        "    exp_inverted_loss_expanded_fil = exp_inverted_loss_expanded\n",
        "    # exp_inverted_loss_expanded_v2_fil = exp_inverted_loss_expanded_v2\n",
        "\n",
        "# Crear rangos de pocas correspondientes\n",
        "epochs_range_regular = range(min_epoch + 1, min_epoch + 1 + len(regular_result_loss_fil))\n",
        "# epochs_range_inverted = range(min_epoch + 1, min_epoch + 1 + len(inverted_result_loss_fil))\n",
        "epochs_range_exp_inverted = range(min_epoch + 1, min_epoch + 1 + len(exp_inverted_result_loss_fil))\n",
        "epochs_range_exp_inverted_expanded = range(min_epoch + 1, min_epoch + 1 + len(exp_inverted_loss_expanded_fil))\n",
        "# epochs_range_exp_inverted_expanded_v2 = range(min_epoch + 1, min_epoch + 1 + len(exp_inverted_loss_expanded_v2_fil))\n",
        "\n",
        "# Crear la grfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plotear los resultados\n",
        "plt.plot(epochs_range_regular, regular_result_loss_fil, label='Regular Loss', marker='o')\n",
        "# plt.plot(epochs_range_inverted, inverted_result_loss_fil, label='Inverted Loss', marker='o')\n",
        "plt.plot(epochs_range_exp_inverted, exp_inverted_result_loss_fil, label='Exp Inverted Loss', marker='o')\n",
        "plt.plot(epochs_range_exp_inverted_expanded, exp_inverted_loss_expanded_fil, label='Exp Inverted Loss Expanded', marker='o')\n",
        "# plt.plot(epochs_range_exp_inverted_expanded_v2, exp_inverted_loss_expanded_v2_fil, label='Exp Inverted Loss 500', marker='o')\n",
        "\n",
        "# Aadir ttulos y etiquetas\n",
        "plt.title('Validation Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar la grfica\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOKfsNHFVxxl"
      },
      "source": [
        "### Data Explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLgFbQBgHPoE",
        "outputId": "69bbd49a-4fc0-4813-e5c3-9270d3d6d061"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de matriz de distancias 2x2\n",
        "distances = np.array([[1000, 8, 2],\n",
        "                      [1, 3, 90]])\n",
        "\n",
        "# Llamar a la funcin calculate_adjacency_matrix con la matriz de distancias de ejemplo\n",
        "adjacency_matrix = calculate_adjacency_matrix(distances, sigma_squared=10, epsilon=0.5)\n",
        "\n",
        "# Imprimir la matriz de adyacencia resultante\n",
        "adjacency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVjaILM1Vxxl"
      },
      "outputs": [],
      "source": [
        "def search_value_in_dataframe(df, value):\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        matching_rows = df[df[col] == value].index.tolist()\n",
        "        for row in matching_rows:\n",
        "            result.append((row, col))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "UgIRn5xKVxxm",
        "outputId": "334cb74e-2b02-4cd5-9f3c-9466e311320c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEHzXf1roqyu"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErvzhxP1oqyu",
        "outputId": "e8fd60c6-ac0e-4e86-81e1-5e709e305681"
      },
      "outputs": [],
      "source": [
        "print(f'x_train size: {x_train_expl.shape}')\n",
        "print(f'y_train size: {y_train_expl.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr-27B_2oqyu",
        "outputId": "a88160a0-824d-48e0-e3de-808b17a3f901"
      },
      "outputs": [],
      "source": [
        "search_value_in_dataframe(data, x_train_expl[23][0][0][0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_sIMUqKoqyu",
        "outputId": "36fbf8d2-0fb9-4485-f1cc-a4d34c590a77"
      },
      "outputs": [],
      "source": [
        "y_train_expl[0][0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNrKo49noqyv",
        "outputId": "48892df3-7b9f-4fb9-c783-b89be0f246ea"
      },
      "outputs": [],
      "source": [
        "search_value_in_dataframe(data, y_train_expl[0][0].item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "syf0-VKoO41E",
        "HtUtFdmjv-DO",
        "p9NhGKrUwCHM",
        "VX3jOPo8v3lv",
        "Q5netoO_O41O",
        "ebTGhcZfO41T",
        "IYYw1guWO41X",
        "iQMUr0E6O41a",
        "x7dPL0rQVxxk",
        "1gM1IMYSO41g",
        "jr8hbDk3O41l",
        "Wt11Kh--O41u",
        "T36na-SMVxxq",
        "GEkcLMedO41s",
        "lnwDDbl7jgB_",
        "w1dJOZbnmdlm"
      ],
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 753534,
          "sourceId": 1302191,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 753767,
          "sourceId": 1302592,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4926209,
          "sourceId": 8292432,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 29962,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
